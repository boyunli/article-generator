import os
from lxml import etree
from urllib.parse import quote
import pandas as pd

from python.requests_pkg import request_get as rget
from python.utils import trim, filter_
from python.settings_dev import logger
from python.pipelines import NewsPipeline

class Wechat():
    '''
    根据搜索关键词进行搜索
    '''

    def parse(self):
        df = self.load_keywords(4)
        size = df.size
        for row in range(0, size):
            keyword = df.loc[row][0]
            for page in range(1, 6):
                url = 'http://weixin.sogou.com/weixin?type=2&query={keyword}&page={page}'\
                    .format(keyword=quote(keyword.encode('utf-8')), page=page)
                logger.debug("\033[92m 开始爬取第{}页，关键词:{} \033[0m".format(page, keyword))
                resp = rget(url)
                if not resp: continue
                html = etree.HTML(resp.content)
                if not html: continue
                hrefs = html.xpath('//ul[@class="news-list"]//li//div[@class="txt-box"]/h3/a/@href')
                if not hrefs: continue
                details = []
                for href in hrefs:
                    try:
                        item = self._extract(href, url)
                        if not item: continue
                        details.append(item)
                    except IndexError:
                        # 像这种很可能是网络原因 导致失败，需要将失败的href写入 某个队列中，待重爬
                        continue
                NewsPipeline().save(details)

    def _extract(self, href, referer):
        resp = rget(href, referer=referer)
        if not resp: return
        html = etree.HTML(resp.content)
        if not html: return

        title = html.xpath('//*[@id="activity-name"]/text()')
        if title:
            title = trim(title[0])
        else:
            return

        publish_time = html.xpath('//*[@id="post-date"]/text()')
        publish_time = publish_time[0] if publish_time else ''
        author = html.xpath('//*[@id="post-user"]/text()')
        author = author[0] if author else ''

        ps = html.xpath('//*[@id="js_content"]//p')
        start_index = 0
        start = ps[start_index].xpath('.//text()')
        if not start:
            start_index = 1
            start = ps[start_index].xpath('.//text()')
        last_index = -1
        last = ps[last_index].xpath('.//text()')
        if not last:
            index = -2
            last = ps[index].xpath('.//text()')

        first = trim(''.join(start))
        sText = [''.join(p.xpath('.//text()'))
                 for p in ps[start_index+1:last_index]
                 if trim(''.join(p.xpath('.//text()')))]
        second = trim('&&&'.join(sText))
        if not second: return
        third = trim(''.join(last))

        if filter_(second): return
        logger.debug('\033[96m title:{}; href:{}; first:{}; second:{}; third:{} \033[0m'
                             .format(title, href, len(first), len(second), len(third)))
        return {
            'category': '手表',
            'site': 'http://weixin.sogou.com/',
            'tag': -1,
            'news_url': href,
            'title': title,
            'first': first,
            'second': second,
            'third': third,
            'author': author,
            'publish_time': publish_time,
        }

    def load_keywords(self, sheet_name):
        file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(
            os.path.abspath(__file__)))), 'resources/keywords.xlsx')
        df = pd.read_excel(file, sheet_name=sheet_name, encoding='gbk', header=1)
        return df.drop_duplicates()


if __name__ == '__main__':
    Wechat().parse()

